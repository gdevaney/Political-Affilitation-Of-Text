{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%cd /data\n",
        "%pip install keybert"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGolnDX84aHe",
        "outputId": "dd0833ae-6033-471f-e913-31b54053d07c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "import string\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "import torch\n",
        "import time\n",
        "from datetime import datetime\n",
        "import ast\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from keybert import KeyBERT\n",
        "\n",
        "from tqdm.notebook import tqdm"
      ],
      "metadata": {
        "id": "Mg-8zAAy0Mxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kwd_model = KeyBERT()\n",
        "bert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
      ],
      "metadata": {
        "id": "HO_o4HR10ttT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bert_encoder(text):\n",
        "    \"\"\" Compute semantic vector with BERT\n",
        "    Parameters\n",
        "    ----------\n",
        "    seq: string to encode\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "        np array\n",
        "    \"\"\"\n",
        "    words = text.split(\" \")\n",
        "    # words = [word for word in words if word in bert_tokenizer.vocab.keys()]\n",
        "    if len(words) > 2048:\n",
        "        words = words[:2048]\n",
        "    n_words = int(np.log2(len(words)))\n",
        "    words = \" \".join(words)\n",
        "    keywords = kwd_model.extract_keywords(words, keyphrase_ngram_range=(1, 3), top_n=n_words)\n",
        "    # keywords2 = kwd_model.extract_keywords(words, keyphrase_ngram_range=(2, 2), top_n=n_words)\n",
        "    # keywords3 = kwd_model.extract_keywords(words, keyphrase_ngram_range=(3, 3), top_n=n_words)\n",
        "    keywords = [word[0] for word in keywords]\n",
        "    # keywords = list(set(keywords))\n",
        "    s = \" \".join(keywords)\n",
        "    s = s + \" \" + text\n",
        "    tokens = bert_tokenizer(s, return_tensors='pt', padding=True, max_length=64, truncation=True)\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "BXZB8kBz0V0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean(seq):\n",
        "    \"\"\" Preprocess sentences for BERT\n",
        "    Parameters\n",
        "    ----------\n",
        "    seq: str, raw sentence\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str, preprocessed sentence\n",
        "    \"\"\"\n",
        "    seq = re.sub('\\]|\\[|\\)|\\(|\\=|\\,|\\;', ' ', seq)\n",
        "    seq = \" \".join([word.lower() if word.isupper() else word for word in seq.strip().split()])\n",
        "    seq = re.sub('([A-Z][a-z]+)', r' \\1', re.sub('([A-Z]+)', r' \\1', seq))\n",
        "    seq = \" \".join([word for word in seq.split() if not bool(re.search(r'\\d', word))])\n",
        "    table = str.maketrans(dict.fromkeys(list(string.punctuation)))\n",
        "    content = seq.translate(table)\n",
        "    seq = \" \".join([word.lower().strip() for word in content.strip().split()])\n",
        "    return seq"
      ],
      "metadata": {
        "id": "7p71lI-60gUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(train, validation):\n",
        "    \"\"\" Load HDFS unstructured log into train and test data\n",
        "    Arguments\n",
        "    ---------\n",
        "        train: str, the file path of training resolutions.\n",
        "        validation: str, the file path of validation resolutions.\n",
        "    Returns\n",
        "    -------\n",
        "        (x_train, y_train): the training data\n",
        "        (x_val, y_val): the validation data\n",
        "    \"\"\"\n",
        "    encoder = bert_encoder\n",
        "\n",
        "    #get data\n",
        "    train_data = pd.read_csv(train)\n",
        "    validation = pd.read_csv(validation)\n",
        "\n",
        "    #convert training data into numpy array\n",
        "    X_train = train_data['text']\n",
        "    X_train = X_train.apply(ast.literal_eval)\n",
        "    X_train = np.array(X_train)\n",
        "    y_train = np.array(train_data['ideology'].tolist())\n",
        "\n",
        "    #filter entries with no assigned ideology\n",
        "    train_inds = np.where(np.isnan(y_train))[0]\n",
        "    mask = np.ones(len(y_train), dtype=bool)\n",
        "    mask[train_inds] = False\n",
        "    y_train = y_train[mask]\n",
        "    X_train = X_train[mask]\n",
        "\n",
        "    #convert validatioin data into numpy array\n",
        "    X_val = validation['text']\n",
        "    X_val = X_val.apply(ast.literal_eval)\n",
        "    X_val = np.array(X_val)\n",
        "    y_val = np.array(validation['ideology'].tolist())\n",
        "\n",
        "    #filter entries with no assigned ideology\n",
        "    val_inds = np.where(np.isnan(y_val))[0]\n",
        "    mask = np.ones(len(y_val), dtype=bool)\n",
        "    mask[val_inds] = False\n",
        "    y_val = y_val[mask]\n",
        "    X_val = X_val[mask]\n",
        "\n",
        "    #convert labels into binary encoding\n",
        "    # y_train = np.around(y_train).astype(float)\n",
        "    # y_val = np.around(y_val).astype(float)\n",
        "\n",
        "    return X_train, y_train, X_val, y_val"
      ],
      "metadata": {
        "id": "3sITjmro0mLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_t, X_val, y_v = load_data('train.csv', 'val.csv')"
      ],
      "metadata": {
        "id": "draDBMw80Vj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_t = []\n",
        "X_t_mask = []\n",
        "#iterate through, clean, and tokenize X_train\n",
        "for i, text in enumerate(X_train):\n",
        "    seq = \" \".join(text)\n",
        "    s = seq.lower()\n",
        "    token = bert_encoder(s)\n",
        "    X_t.append(token['input_ids'].squeeze())\n",
        "    X_t_mask.append(token['attention_mask'].squeeze())\n",
        "    if i % 100 == 0:\n",
        "        print(i)\n",
        "\n",
        "X_v = []\n",
        "X_v_mask = []\n",
        "#iterate through, clean, and tokenize X_val\n",
        "for i, text in enumerate(X_val):\n",
        "    seq = \" \".join(text)\n",
        "    s = seq.lower()\n",
        "    token = bert_encoder(s)\n",
        "    X_v.append(token['input_ids'].squeeze())\n",
        "    X_v_mask.append(token['attention_mask'].squeeze())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kmun6FPI0aN3",
        "outputId": "bdbf6ec8-c7ea-4e4e-f396-9058097757a9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pad list of tensors to the same size so they can be tensorized\n",
        "X_t = pad_sequence(X_t, batch_first=True, padding_value=0)\n",
        "X_t_mask = pad_sequence(X_t_mask, batch_first=True, padding_value=0)\n",
        "\n",
        "X_v = pad_sequence(X_v, batch_first=True, padding_value=0)\n",
        "X_v_mask = pad_sequence(X_v_mask, batch_first=True, padding_value=0)"
      ],
      "metadata": {
        "id": "sst6I9qE0dHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert list of tokens to tensors\n",
        "X_train = torch.tensor(X_t)\n",
        "X_val = torch.tensor(X_v)\n",
        "\n",
        "X_train_mask = torch.tensor(X_t_mask)\n",
        "X_val_mask = torch.tensor(X_v_mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUkqb9FD0fnH",
        "outputId": "c44f20bd-0930-47ba-cb93-5b9fae3cd3e4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert labels to tensors\n",
        "y_train = torch.tensor(y_t)\n",
        "y_val = torch.tensor(y_v)"
      ],
      "metadata": {
        "id": "w0T5S9Hs0jSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
        "from sklearn.utils import shuffle"
      ],
      "metadata": {
        "id": "9zHDeVpP0lug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerClassifier(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(TransformerClassifier, self).__init__()\n",
        "        self.bert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.fc = nn.Linear(768, num_classes)  # BERT base model output size is 768\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = torch.mean(output.last_hidden_state, dim=1)\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.fc(pooled_output)\n",
        "        logits = self.sigmoid(logits)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "KPRL_GnY0o4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TransformerClassifier(num_classes=2)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "train_dataset = TensorDataset(X_train, X_train_mask, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "val_dataset = TensorDataset(X_val, X_val_mask, y_val)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "\n",
        "for epoch in range(20):\n",
        "    model.train()\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids, attention_mask, target = batch\n",
        "        logits = model(input_ids, attention_mask)\n",
        "        probs = logits[:,1].float()\n",
        "        # target = torch.round(target)\n",
        "        loss = loss_fn(probs, target.float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_acc = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids, attention_mask, target = batch\n",
        "            logits = model(input_ids, attention_mask)\n",
        "            probs = logits[:,1].float()\n",
        "            # target = torch.round(target)\n",
        "            val_loss += loss_fn(probs, target.float()).item()\n",
        "            val_acc += (logits.argmax(dim=1) == np.around(target)).sum().item()\n",
        "    val_loss /= len(val_loader)\n",
        "    val_acc /= len(val_loader.dataset)\n",
        "    print(f\"Epoch {epoch+1}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rw8ry2Ul0rTr",
        "outputId": "399abf1d-90c0-40fc-eb44-66a5b3fdee02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_test_data(test):\n",
        "    \"\"\" Load HDFS unstructured log into train and test data\n",
        "    Arguments\n",
        "    ---------\n",
        "        test: str, the file path of test resolutions.\n",
        "    Returns\n",
        "    -------\n",
        "        (x_test, y_test): the test data\n",
        "    \"\"\"\n",
        "    encoder = bert_encoder\n",
        "\n",
        "    #get data\n",
        "    test_data = pd.read_csv(test)\n",
        "\n",
        "    #convert training data into numpy array\n",
        "    X_test = test_data['text']\n",
        "    X_test = X_test.apply(ast.literal_eval)\n",
        "    X_test = np.array(X_test)\n",
        "    y_test = np.array(test_data['ideology'].tolist())\n",
        "\n",
        "    #filter entries with no assigned ideology\n",
        "    test_inds = np.where(np.isnan(y_test))[0]\n",
        "    mask = np.ones(len(y_test), dtype=bool)\n",
        "    mask[test_inds] = False\n",
        "    y_test = y_test[mask]\n",
        "    X_test = X_test[mask]\n",
        "\n",
        "    #convert labels into binary encoding\n",
        "    # y_train = np.around(y_train).astype(float)\n",
        "    # y_val = np.around(y_val).astype(float)\n",
        "\n",
        "    return X_test, y_test"
      ],
      "metadata": {
        "id": "2C6zMXCY7xHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_t, y_test = load_test_data('test.csv')"
      ],
      "metadata": {
        "id": "J_n_dlE3A8hi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = []\n",
        "X_test_mask = []\n",
        "#iterate through, clean, and tokenize X_test\n",
        "for i, text in enumerate(X_t):\n",
        "    seq = \" \".join(text)\n",
        "    s = clean(seq).lower()\n",
        "    token = bert_encoder(s)\n",
        "    X_test.append(token['input_ids'].squeeze())\n",
        "    X_test_mask.append(token['attention_mask'].squeeze())\n",
        "    if i % 100 == 0:\n",
        "        print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0baxNZbBhT6",
        "outputId": "9319406a-7f3c-4d74-f207-97111808f636"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = pad_sequence(X_test, batch_first=True, padding_value=0)\n",
        "X_test_mask = pad_sequence(X_test_mask, batch_first=True, padding_value=0)"
      ],
      "metadata": {
        "id": "nTtEmUaGBiMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = torch.tensor(X_test)\n",
        "X_test_mask = torch.tensor(X_test_mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TX377hA4BwjI",
        "outputId": "1dedf258-5003-4523-b162-01e2689a5364"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test = torch.tensor(y_test)"
      ],
      "metadata": {
        "id": "icd0lCH0Dkli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = TensorDataset(X_test, X_test_mask, y_test)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "pH03pzPgFR7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = []\n",
        "true = []\n",
        "binary_preds = []\n",
        "true_class = []\n",
        "for batch in test_loader:\n",
        "    input_ids, attention_mask, target = batch\n",
        "    logits = model(input_ids, attention_mask)\n",
        "    probs = logits[:,1].float()\n",
        "    preds.extend(probs)\n",
        "    true.extend(target)\n",
        "    binary_preds.extend(logits.argmax(dim=1))\n",
        "    true_class.extend(np.around(target))"
      ],
      "metadata": {
        "id": "DXyy6Sk-Doda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tensors_to_list(list_of_tensors):\n",
        "    list_of_arrays = [round(tensor.item(), 4) for tensor in list_of_tensors]\n",
        "    return list_of_arrays\n"
      ],
      "metadata": {
        "id": "_TewavgWE9SL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = tensors_to_list(preds)\n",
        "true = tensors_to_list(true)\n",
        "binary_preds = tensors_to_list(binary_preds)\n",
        "true_class = tensors_to_list(true_class)"
      ],
      "metadata": {
        "id": "A4j_yUc4Ec7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(true_class, binary_preds))"
      ],
      "metadata": {
        "id": "B5ikUc4_Fd7m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
