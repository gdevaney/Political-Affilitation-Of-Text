{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJGSDkVh0JIj",
        "outputId": "a47d5c00-91b1-4538-8f27-208bfb0ff8ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/.shortcut-targets-by-id/1XAIHXAym6w3aZynQYubNhZ-kR2ByPq8C/NLP Final Project\n",
            " BERT.ipynb                                 sres.gsheet\n",
            " \u001b[0m\u001b[01;34mData\u001b[0m/                                      test.csv\n",
            " data_preprocessing.ipynb                   Testing_Bill_Parsing-Garrett.ipynb\n",
            " distilBERT_20.pth                          test.ipynb\n",
            " DistilBERT_Model.ipynb                     train_18-22.csv\n",
            " \u001b[01;34mfinetuning-sentiment-model-3000-samples\u001b[0m/   train2.csv\n",
            " GPT.csv                                    train.csv\n",
            " GPT.gsheet                                 train.gsheet\n",
            "'GPT Input.gdoc'                            val_18-22.csv\n",
            " GPT.ipynb                                  val2.csv\n",
            " hres.csv                                   val2.gsheet\n",
            " KeyBERT.ipynb                              val.csv\n",
            " Lisa_count_tf-idf.ipynb                    val.gsheet\n",
            " sbill.csv                                  XLNET.ipynb\n",
            " sbill.gsheet                               xlnet_large_55.pth\n",
            "'sres (1).gsheet'                           xlnet_train2_55.pth\n",
            " sres.csv\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive//NLP Final Project\n",
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install keybert"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGolnDX84aHe",
        "outputId": "dd0833ae-6033-471f-e913-31b54053d07c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keybert in /usr/local/lib/python3.10/dist-packages (0.8.4)\n",
            "Requirement already satisfied: sentence-transformers>=0.3.8 in /usr/local/lib/python3.10/dist-packages (from keybert) (2.7.0)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.10/dist-packages (from keybert) (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from keybert) (1.25.2)\n",
            "Requirement already satisfied: rich>=10.4.0 in /usr/local/lib/python3.10/dist-packages (from keybert) (13.7.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.4.0->keybert) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.4.0->keybert) (2.16.1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2->keybert) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2->keybert) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2->keybert) (3.4.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (4.40.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (4.66.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (2.2.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (0.20.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.3.8->keybert) (3.13.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.3.8->keybert) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.3.8->keybert) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.3.8->keybert) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.3.8->keybert) (4.11.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.3.8->keybert) (24.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.4.0->keybert) (0.1.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.4.127)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers>=0.3.8->keybert) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers>=0.3.8->keybert) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers>=0.3.8->keybert) (0.4.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.3.8->keybert) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.3.8->keybert) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.3.8->keybert) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.3.8->keybert) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "import re\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "import string\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "import torch\n",
        "import time\n",
        "from datetime import datetime\n",
        "import ast\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from keybert import KeyBERT\n",
        "\n",
        "from tqdm.notebook import tqdm"
      ],
      "metadata": {
        "id": "Mg-8zAAy0Mxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kwd_model = KeyBERT()\n",
        "bert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
      ],
      "metadata": {
        "id": "HO_o4HR10ttT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bert_encoder(text):\n",
        "    \"\"\" Compute semantic vector with BERT\n",
        "    Parameters\n",
        "    ----------\n",
        "    seq: string to encode\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "        np array\n",
        "    \"\"\"\n",
        "    words = text.split(\" \")\n",
        "    # words = [word for word in words if word in bert_tokenizer.vocab.keys()]\n",
        "    if len(words) > 2048:\n",
        "        words = words[:2048]\n",
        "    n_words = int(np.log2(len(words)))\n",
        "    words = \" \".join(words)\n",
        "    keywords = kwd_model.extract_keywords(words, keyphrase_ngram_range=(1, 3), top_n=n_words)\n",
        "    # keywords2 = kwd_model.extract_keywords(words, keyphrase_ngram_range=(2, 2), top_n=n_words)\n",
        "    # keywords3 = kwd_model.extract_keywords(words, keyphrase_ngram_range=(3, 3), top_n=n_words)\n",
        "    keywords = [word[0] for word in keywords]\n",
        "    # keywords = list(set(keywords))\n",
        "    s = \" \".join(keywords)\n",
        "    s = s + \" \" + text\n",
        "    tokens = bert_tokenizer(s, return_tensors='pt', padding=True, max_length=64, truncation=True)\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "BXZB8kBz0V0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean(seq):\n",
        "    \"\"\" Preprocess sentences for BERT\n",
        "    Parameters\n",
        "    ----------\n",
        "    seq: str, raw sentence\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str, preprocessed sentence\n",
        "    \"\"\"\n",
        "    seq = re.sub('\\]|\\[|\\)|\\(|\\=|\\,|\\;', ' ', seq)\n",
        "    seq = \" \".join([word.lower() if word.isupper() else word for word in seq.strip().split()])\n",
        "    seq = re.sub('([A-Z][a-z]+)', r' \\1', re.sub('([A-Z]+)', r' \\1', seq))\n",
        "    seq = \" \".join([word for word in seq.split() if not bool(re.search(r'\\d', word))])\n",
        "    table = str.maketrans(dict.fromkeys(list(string.punctuation)))\n",
        "    content = seq.translate(table)\n",
        "    seq = \" \".join([word.lower().strip() for word in content.strip().split()])\n",
        "    return seq"
      ],
      "metadata": {
        "id": "7p71lI-60gUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(train, validation):\n",
        "    \"\"\" Load HDFS unstructured log into train and test data\n",
        "    Arguments\n",
        "    ---------\n",
        "        train: str, the file path of training resolutions.\n",
        "        validation: str, the file path of validation resolutions.\n",
        "    Returns\n",
        "    -------\n",
        "        (x_train, y_train): the training data\n",
        "        (x_val, y_val): the validation data\n",
        "    \"\"\"\n",
        "    encoder = bert_encoder\n",
        "\n",
        "    #get data\n",
        "    train_data = pd.read_csv(train)\n",
        "    validation = pd.read_csv(validation)\n",
        "\n",
        "    #convert training data into numpy array\n",
        "    X_train = train_data['text']\n",
        "    X_train = X_train.apply(ast.literal_eval)\n",
        "    X_train = np.array(X_train)\n",
        "    y_train = np.array(train_data['ideology'].tolist())\n",
        "\n",
        "    #filter entries with no assigned ideology\n",
        "    train_inds = np.where(np.isnan(y_train))[0]\n",
        "    mask = np.ones(len(y_train), dtype=bool)\n",
        "    mask[train_inds] = False\n",
        "    y_train = y_train[mask]\n",
        "    X_train = X_train[mask]\n",
        "\n",
        "    #convert validatioin data into numpy array\n",
        "    X_val = validation['text']\n",
        "    X_val = X_val.apply(ast.literal_eval)\n",
        "    X_val = np.array(X_val)\n",
        "    y_val = np.array(validation['ideology'].tolist())\n",
        "\n",
        "    #filter entries with no assigned ideology\n",
        "    val_inds = np.where(np.isnan(y_val))[0]\n",
        "    mask = np.ones(len(y_val), dtype=bool)\n",
        "    mask[val_inds] = False\n",
        "    y_val = y_val[mask]\n",
        "    X_val = X_val[mask]\n",
        "\n",
        "    #convert labels into binary encoding\n",
        "    # y_train = np.around(y_train).astype(float)\n",
        "    # y_val = np.around(y_val).astype(float)\n",
        "\n",
        "    return X_train, y_train, X_val, y_val"
      ],
      "metadata": {
        "id": "3sITjmro0mLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_t, X_val, y_v = load_data('train.csv', 'val.csv')"
      ],
      "metadata": {
        "id": "draDBMw80Vj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_t = []\n",
        "X_t_mask = []\n",
        "#iterate through, clean, and tokenize X_train\n",
        "for i, text in enumerate(X_train):\n",
        "    seq = \" \".join(text)\n",
        "    s = seq.lower()\n",
        "    token = bert_encoder(s)\n",
        "    X_t.append(token['input_ids'].squeeze())\n",
        "    X_t_mask.append(token['attention_mask'].squeeze())\n",
        "    if i % 100 == 0:\n",
        "        print(i)\n",
        "\n",
        "X_v = []\n",
        "X_v_mask = []\n",
        "#iterate through, clean, and tokenize X_val\n",
        "for i, text in enumerate(X_val):\n",
        "    seq = \" \".join(text)\n",
        "    s = seq.lower()\n",
        "    token = bert_encoder(s)\n",
        "    X_v.append(token['input_ids'].squeeze())\n",
        "    X_v_mask.append(token['attention_mask'].squeeze())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kmun6FPI0aN3",
        "outputId": "bdbf6ec8-c7ea-4e4e-f396-9058097757a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n",
            "1100\n",
            "1200\n",
            "1300\n",
            "1400\n",
            "1500\n",
            "1600\n",
            "1700\n",
            "1800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pad list of tensors to the same size so they can be tensorized\n",
        "X_t = pad_sequence(X_t, batch_first=True, padding_value=0)\n",
        "X_t_mask = pad_sequence(X_t_mask, batch_first=True, padding_value=0)\n",
        "\n",
        "X_v = pad_sequence(X_v, batch_first=True, padding_value=0)\n",
        "X_v_mask = pad_sequence(X_v_mask, batch_first=True, padding_value=0)"
      ],
      "metadata": {
        "id": "sst6I9qE0dHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert list of tokens to tensors\n",
        "X_train = torch.tensor(X_t)\n",
        "X_val = torch.tensor(X_v)\n",
        "\n",
        "X_train_mask = torch.tensor(X_t_mask)\n",
        "X_val_mask = torch.tensor(X_v_mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUkqb9FD0fnH",
        "outputId": "c44f20bd-0930-47ba-cb93-5b9fae3cd3e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-62-f74968e82929>:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X_train = torch.tensor(X_t)\n",
            "<ipython-input-62-f74968e82929>:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X_val = torch.tensor(X_v)\n",
            "<ipython-input-62-f74968e82929>:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X_train_mask = torch.tensor(X_t_mask)\n",
            "<ipython-input-62-f74968e82929>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X_val_mask = torch.tensor(X_v_mask)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#convert labels to tensors\n",
        "y_train = torch.tensor(y_t)\n",
        "y_val = torch.tensor(y_v)"
      ],
      "metadata": {
        "id": "w0T5S9Hs0jSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
        "from sklearn.utils import shuffle"
      ],
      "metadata": {
        "id": "9zHDeVpP0lug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerClassifier(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(TransformerClassifier, self).__init__()\n",
        "        self.bert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.fc = nn.Linear(768, num_classes)  # BERT base model output size is 768\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = torch.mean(output.last_hidden_state, dim=1)\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.fc(pooled_output)\n",
        "        logits = self.sigmoid(logits)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "KPRL_GnY0o4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TransformerClassifier(num_classes=2)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "train_dataset = TensorDataset(X_train, X_train_mask, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "val_dataset = TensorDataset(X_val, X_val_mask, y_val)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "\n",
        "for epoch in range(20):\n",
        "    model.train()\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids, attention_mask, target = batch\n",
        "        logits = model(input_ids, attention_mask)\n",
        "        probs = logits[:,1].float()\n",
        "        # target = torch.round(target)\n",
        "        loss = loss_fn(probs, target.float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_acc = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids, attention_mask, target = batch\n",
        "            logits = model(input_ids, attention_mask)\n",
        "            probs = logits[:,1].float()\n",
        "            # target = torch.round(target)\n",
        "            val_loss += loss_fn(probs, target.float()).item()\n",
        "            val_acc += (logits.argmax(dim=1) == np.around(target)).sum().item()\n",
        "    val_loss /= len(val_loader)\n",
        "    val_acc /= len(val_loader.dataset)\n",
        "    print(f\"Epoch {epoch+1}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rw8ry2Ul0rTr",
        "outputId": "399abf1d-90c0-40fc-eb44-66a5b3fdee02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Val Loss: 0.0591, Val Acc: 0.6881\n",
            "Epoch 2, Val Loss: 0.0544, Val Acc: 0.7464\n",
            "Epoch 3, Val Loss: 0.0581, Val Acc: 0.7360\n",
            "Epoch 4, Val Loss: 0.0575, Val Acc: 0.7609\n",
            "Epoch 5, Val Loss: 0.0551, Val Acc: 0.7630\n",
            "Epoch 6, Val Loss: 0.0579, Val Acc: 0.7505\n",
            "Epoch 7, Val Loss: 0.0543, Val Acc: 0.7630\n",
            "Epoch 8, Val Loss: 0.0537, Val Acc: 0.7464\n",
            "Epoch 9, Val Loss: 0.0555, Val Acc: 0.7505\n",
            "Epoch 10, Val Loss: 0.0574, Val Acc: 0.7568\n",
            "Epoch 11, Val Loss: 0.0578, Val Acc: 0.7505\n",
            "Epoch 12, Val Loss: 0.0559, Val Acc: 0.7422\n",
            "Epoch 13, Val Loss: 0.0580, Val Acc: 0.7443\n",
            "Epoch 14, Val Loss: 0.0551, Val Acc: 0.7547\n",
            "Epoch 15, Val Loss: 0.0563, Val Acc: 0.7464\n",
            "Epoch 16, Val Loss: 0.0549, Val Acc: 0.7484\n",
            "Epoch 17, Val Loss: 0.0566, Val Acc: 0.7505\n",
            "Epoch 18, Val Loss: 0.0550, Val Acc: 0.7526\n",
            "Epoch 19, Val Loss: 0.0555, Val Acc: 0.7484\n",
            "Epoch 20, Val Loss: 0.0583, Val Acc: 0.7401\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_test_data(test):\n",
        "    \"\"\" Load HDFS unstructured log into train and test data\n",
        "    Arguments\n",
        "    ---------\n",
        "        test: str, the file path of test resolutions.\n",
        "    Returns\n",
        "    -------\n",
        "        (x_test, y_test): the test data\n",
        "    \"\"\"\n",
        "    encoder = bert_encoder\n",
        "\n",
        "    #get data\n",
        "    test_data = pd.read_csv(test)\n",
        "\n",
        "    #convert training data into numpy array\n",
        "    X_test = test_data['text']\n",
        "    X_test = X_test.apply(ast.literal_eval)\n",
        "    X_test = np.array(X_test)\n",
        "    y_test = np.array(test_data['ideology'].tolist())\n",
        "\n",
        "    #filter entries with no assigned ideology\n",
        "    test_inds = np.where(np.isnan(y_test))[0]\n",
        "    mask = np.ones(len(y_test), dtype=bool)\n",
        "    mask[test_inds] = False\n",
        "    y_test = y_test[mask]\n",
        "    X_test = X_test[mask]\n",
        "\n",
        "    #convert labels into binary encoding\n",
        "    # y_train = np.around(y_train).astype(float)\n",
        "    # y_val = np.around(y_val).astype(float)\n",
        "\n",
        "    return X_test, y_test"
      ],
      "metadata": {
        "id": "2C6zMXCY7xHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_t, y_test = load_test_data('test.csv')"
      ],
      "metadata": {
        "id": "J_n_dlE3A8hi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = []\n",
        "X_test_mask = []\n",
        "#iterate through, clean, and tokenize X_test\n",
        "for i, text in enumerate(X_t):\n",
        "    seq = \" \".join(text)\n",
        "    s = clean(seq).lower()\n",
        "    token = bert_encoder(s)\n",
        "    X_test.append(token['input_ids'].squeeze())\n",
        "    X_test_mask.append(token['attention_mask'].squeeze())\n",
        "    if i % 100 == 0:\n",
        "        print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0baxNZbBhT6",
        "outputId": "9319406a-7f3c-4d74-f207-97111808f636"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = pad_sequence(X_test, batch_first=True, padding_value=0)\n",
        "X_test_mask = pad_sequence(X_test_mask, batch_first=True, padding_value=0)"
      ],
      "metadata": {
        "id": "nTtEmUaGBiMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = torch.tensor(X_test)\n",
        "X_test_mask = torch.tensor(X_test_mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TX377hA4BwjI",
        "outputId": "1dedf258-5003-4523-b162-01e2689a5364"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-71-1c1b3c71ef1b>:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X_test = torch.tensor(X_test)\n",
            "<ipython-input-71-1c1b3c71ef1b>:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X_test_mask = torch.tensor(X_test_mask)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test = torch.tensor(y_test)"
      ],
      "metadata": {
        "id": "icd0lCH0Dkli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = TensorDataset(X_test, X_test_mask, y_test)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "pH03pzPgFR7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = []\n",
        "true = []\n",
        "binary_preds = []\n",
        "true_class = []\n",
        "for batch in test_loader:\n",
        "    input_ids, attention_mask, target = batch\n",
        "    logits = model(input_ids, attention_mask)\n",
        "    probs = logits[:,1].float()\n",
        "    preds.extend(probs)\n",
        "    true.extend(target)\n",
        "    binary_preds.extend(logits.argmax(dim=1))\n",
        "    true_class.extend(np.around(target))"
      ],
      "metadata": {
        "id": "DXyy6Sk-Doda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tensors_to_list(list_of_tensors):\n",
        "    list_of_arrays = [round(tensor.item(), 4) for tensor in list_of_tensors]\n",
        "    return list_of_arrays\n"
      ],
      "metadata": {
        "id": "_TewavgWE9SL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = tensors_to_list(preds)\n",
        "true = tensors_to_list(true)\n",
        "binary_preds = tensors_to_list(binary_preds)\n",
        "true_class = tensors_to_list(true_class)"
      ],
      "metadata": {
        "id": "A4j_yUc4Ec7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(true_class, binary_preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JITzWAK_EjxR",
        "outputId": "9a95e234-8b7f-43b9-e807-2e14e1f36379"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.65      0.77      0.70       257\n",
            "         1.0       0.57      0.41      0.48       186\n",
            "\n",
            "    accuracy                           0.62       443\n",
            "   macro avg       0.61      0.59      0.59       443\n",
            "weighted avg       0.61      0.62      0.61       443\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "comparison = pd.DataFrame(columns=['true', 'pred'])\n",
        "comparison['true'] = true\n",
        "comparison['pred'] = preds"
      ],
      "metadata": {
        "id": "Z0boodjgHfi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mse(true, pred):\n",
        "    return np.mean((true - pred)**2)"
      ],
      "metadata": {
        "id": "xGp3OQvBJBIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mse(np.array(true), np.array(preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EM5l8GPzJEK0",
        "outputId": "45965811-77bd-4d80-c9c5-178452af4533"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.10106868501128669"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SGhZnmdLJZIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv(\"train.csv\")\n",
        "test = pd.read_csv(\"val.csv\")\n",
        "\n",
        "# test_true = test['ideology'].values\n",
        "\n",
        "train['ideology'].loc[train['ideology'] > .5] = 1 # convert to strict scores of 1\n",
        "train['ideology'].loc[train['ideology'] <= .5] = 0\n",
        "\n",
        "test['ideology'].loc[test['ideology'] > .5] = 1 # convert to strict scores of 1\n",
        "test['ideology'].loc[test['ideology'] <= .5] = 0\n",
        "\n",
        "train.dropna(axis='rows', inplace=True)\n",
        "\n",
        "test.dropna(axis='rows', inplace=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMZVUb-HJpPT",
        "outputId": "63dc07ee-8283-429b-bd22-97391fdf86d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-81-a7591d60ef8c>:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train['ideology'].loc[train['ideology'] > .5] = 1 # convert to strict scores of 1\n",
            "<ipython-input-81-a7591d60ef8c>:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train['ideology'].loc[train['ideology'] <= .5] = 0\n",
            "<ipython-input-81-a7591d60ef8c>:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  test['ideology'].loc[test['ideology'] > .5] = 1 # convert to strict scores of 1\n",
            "<ipython-input-81-a7591d60ef8c>:10: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  test['ideology'].loc[test['ideology'] <= .5] = 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = train['text'].values\n",
        "y_train = train['ideology'].values\n",
        "\n",
        "X_test = test['text'].values\n",
        "y_test = test['ideology'].values"
      ],
      "metadata": {
        "id": "hrTYXRrdcZ6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWvLNdvhhp5C",
        "outputId": "5287b59a-2fcb-46e8-a0ae-cf8de293f5da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['A resolution commemorating the 48th anniversary of the signing into law of the Employee Retirement Income Security Act of 1974, recognizing the importance the Employee Retirement Income Security Act of 1974 plays in protecting the interests of participants in employee benefit plans and their beneficiaries, and recognizing the need to encourage more workers to participate in such plans to save for their retirement.', 'Commemorating the 48th anniversary of the signing into law of the Employee Retirement Income Security Act of 1974, recognizing the importance the Employee Retirement Income Security Act of 1974 plays in protecting the interests of participants in employee benefit plans and their beneficiaries, and recognizing the need to encourage more workers to participate in such plans to save for their retirement.Whereas September 2, 2022, marks the 48th anniversary of the signing into law of the Employee Retirement Income Security Act of 1974 (29 U.S.C. 1001 et seq.) (referred to in this preamble as ERISA)', 'Whereas the purpose of ERISA is to protect the interests of participants in employee benefit plans and their beneficiaries', 'Whereas, since ERISA became law, the number of employee benefit plans has nearly tripled from approximately 300,000 in 1974 to 730,000 in 2019', 'Whereas, during that same time period, the number of participants in employee benefit plans has increased more than 200 percent from approximately 45,000,000 in 1974 to 142,000,000 in 2019', 'Whereas employee benefit plans under ERISA continue to be an important factor affecting the stability of employment and the well-being and financial security of millions of workers and their dependents', 'Whereas, despite the enormous growth in employee benefit plans, only 56 percent of workers participate in such plans', 'Whereas Congress is working to pass comprehensive retirement legislation to encourage more employers to offer robust employee benefit plans and help more workers to participate in such plans', ' andWhereas ERISA will play a critical role in protecting these additional participants and their beneficiaries: Now, therefore, be itThat the Senateâ€”(1)recognizes the importance the Employee Retirement Income Security Act of 1974 (29 U.S.C. 1001 et seq.) plays in protecting the interests of participants in employee benefit plans and their beneficiaries', ' and(2)recognizes the work that still remains to be done to encourage more workers to participate in such plans to save for their retirement. ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vec = TfidfVectorizer()\n",
        "vec.fit(X_train)\n",
        "x_train=vec.transform(X_train)\n",
        "x_test=vec.transform(X_test)"
      ],
      "metadata": {
        "id": "aM5J436iaO28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "lr = LogisticRegression()\n",
        "lr.fit(x_train, y_train)\n",
        "\n",
        "score = lr.score(x_test, y_test)\n",
        "\n",
        "print(\"LR score:\", score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uybx89zha_dS",
        "outputId": "385f43f5-6c4b-4ae8-f8cb-6a740473281d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LR score: 0.7837837837837838\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test2 = pd.read_csv(\"test.csv\")\n",
        "test2['ideology'].loc[test2['ideology'] > .5] = 1 # convert to strict scores of 1\n",
        "test2['ideology'].loc[test2['ideology'] <= .5] = 0\n",
        "X_test2 = test2['text'].values\n",
        "y_test2 = test2['ideology'].values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0fpOEoWbF1V",
        "outputId": "d6163b44-34f5-4c17-c420-2e6bc01dd565"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-102-5f5f8e72fbb7>:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  test2['ideology'].loc[test2['ideology'] > .5] = 1 # convert to strict scores of 1\n",
            "<ipython-input-102-5f5f8e72fbb7>:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  test2['ideology'].loc[test2['ideology'] <= .5] = 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test2=vec.transform(X_test2)"
      ],
      "metadata": {
        "id": "nQVkvVhibJte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds2 = lr.predict(x_test2)"
      ],
      "metadata": {
        "id": "qT_Sa_JUeJ6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mse(y_test2, preds2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVUHjjoW27eF",
        "outputId": "954fb690-69b7-4108-f873-7d82136639c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4018058690744921"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test2, preds2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "navkj7uzeL5q",
        "outputId": "7bfbd5a8-8d1c-4c4e-e7dc-d2d7e1d10dc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.60      0.94      0.73       257\n",
            "         1.0       0.60      0.13      0.21       186\n",
            "\n",
            "    accuracy                           0.60       443\n",
            "   macro avg       0.60      0.53      0.47       443\n",
            "weighted avg       0.60      0.60      0.51       443\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(16, input_dim=x_train.shape[1], activation='relu'))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy',optimizer='Adam',metrics=['accuracy'])\n",
        "\n"
      ],
      "metadata": {
        "id": "o9cvacdueYAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vec = CountVectorizer()\n",
        "vec.fit(X_train)\n",
        "x_train=vec.transform(X_train)\n",
        "x_test=vec.transform(X_test)\n",
        "\n",
        "model.summary()\n",
        "model.fit(x_train, y_train)\n",
        "results = model.evaluate(x_test, y_test, batch_size=128)\n",
        "print(\"test loss, test acc:\", results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GgZrXWJnfQY4",
        "outputId": "8f1bad6d-2dda-4cb2-a2e6-edfda769d094"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_3 (Dense)             (None, 16)                756560    \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 16)                272       \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 756849 (2.89 MB)\n",
            "Trainable params: 756849 (2.89 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "60/60 [==============================] - 12s 174ms/step - loss: 0.5969 - accuracy: 0.6755\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5464 - accuracy: 0.6632\n",
            "test loss, test acc: [0.5464080572128296, 0.6632016897201538]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test2 = pd.read_csv(\"test.csv\")\n",
        "# test2['ideology'].loc[test2['ideology'] > .5] = 1 # convert to strict scores of 1\n",
        "# test2['ideology'].loc[test2['ideology'] <= .5] = 0\n",
        "X_test2 = test2['text'].values\n",
        "y_test2 = test2['ideology'].values"
      ],
      "metadata": {
        "id": "7lgN32dcfUML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test2=vec.transform(X_test2)"
      ],
      "metadata": {
        "id": "amy1CV5g8Oyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds2 = model.predict(x_test2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpHAvUzJ8UoD",
        "outputId": "daa9f454-6f97-44a3-f6b4-5918347b77d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14/14 [==============================] - 0s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test2, np.around(preds2)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "uqviUNKP8YsV",
        "outputId": "1ead2500-bae7-47e5-ac71-5e3b1b9a2a3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Classification metrics can't handle a mix of continuous and binary targets",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-112-498aea7fdbd5>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   2308\u001b[0m     \"\"\"\n\u001b[1;32m   2309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2310\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2312\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m     96\u001b[0m             \"Classification metrics can't handle a mix of {0} and {1} targets\".format(\n\u001b[1;32m     97\u001b[0m                 \u001b[0mtype_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of continuous and binary targets"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mse(y_test2, preds2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-0FSRxZ8esx",
        "outputId": "e3adfbc3-9e49-4599-9d0d-a20deedcff53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.10696556023019943"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QZk4rGls9N7w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}